# -*- coding: utf-8 -*-
"""DataSetDivider.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Z-xfulG7fqrydHDLJeuZFBbbBRIeOxx
"""

import numpy as np

# Cargar el archivo .npy original
data = np.load('output_df_reduce.npy')

# Pedir al usuario el número de fragmentos en los que dividir el archivo
n_fragments = int(input('En cuántos fragmentos quieres partir el archivo? '))

# Calcular el tamaño de cada fragmento
fragment_size = len(data) // n_fragments

# Crear una lista vacía para almacenar los fragmentos
fragments = []

# Dividir el archivo en fragmentos
for i in range(n_fragments):
    start = i * fragment_size
    end = start + fragment_size
    fragment = data[start:end]
    fragments.append(fragment)

# Guardar cada fragmento como un nuevo archivo .npy
for i, fragment in enumerate(fragments):
    np.save(f'input_df_reduce_{i}.npy', fragment)

from sklearn.model_selection import train_test_split
import numpy as np

data = np.load('output_df_reduce.npy')
print(len(data))

training_output, testing_output = train_test_split(data, test_size=0.2, random_state=25)

print(f"No. of training examples: {training_output.shape[0]}")
print(f"No. of testing examples: {testing_output.shape[0]}")

from sklearn.model_selection import train_test_split
import numpy as np

data = np.load('input_df_reduce.npy')
print(len(data))

training_input, testing_input = train_test_split(data, test_size=0.2, random_state=25)

print(f"No. of training examples: {training_input.shape[0]}")
print(f"No. of testing examples: {testing_input.shape[0]}")

print(np.unique(training_output,return_counts=True))
print(np.unique(testing_output,return_counts=True))

